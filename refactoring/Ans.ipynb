{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model.py\n",
    "## Exmaple of Model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(1024, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = x.view(-1, 1024)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def cnn():\n",
    "    return CNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x22d44e368b0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x22d44e36bb0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class loader(object):\n",
    "    def __init__(self, batch_size=64, type=\"NON_IID\"):\n",
    "        self.batch_size = batch_size\n",
    "        self.type = type\n",
    "        self.__load_dataset()\n",
    "        self.__get_index()\n",
    "\n",
    "    def __load_dataset(self):\n",
    "        # mnist\n",
    "        self.train_mnist = datasets.MNIST('./dataset/',\n",
    "                                          train=True,\n",
    "                                          download=True,\n",
    "                                          transform=transforms.Compose([\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                          ]))\n",
    "\n",
    "        self.test_mnist = datasets.MNIST('./dataset/',\n",
    "                                         train=False,\n",
    "                                         download=True,\n",
    "                                         transform=transforms.Compose([\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                         ]))\n",
    "\n",
    "    def __get_index(self):\n",
    "        self.train_dataset = self.train_mnist\n",
    "        self.test_dataset = self.test_mnist\n",
    "\n",
    "        self.indices = [[], [], [], [], [], [], [], [], [], []]\n",
    "        for index, data in enumerate(self.train_dataset):\n",
    "            self.indices[data[1]].append(index)\n",
    "\n",
    "    def get_loader(self, rank):\n",
    "        if not rank:\n",
    "            rank = np.random.randint(10)\n",
    "        else:\n",
    "            rank = int(rank[0])\n",
    "            np.random.seed(rank)\n",
    "        \n",
    "        if self.type == \"NON_IID\":\n",
    "            num_classes = 10\n",
    "            shards_per_class = 2\n",
    "            total_shards = num_classes * shards_per_class\n",
    "            shard_size = len(self.indices[0]) // shards_per_class\n",
    "\n",
    "            sorted_indices = [sorted(self.indices[i]) for i in range(num_classes)]  \n",
    "\n",
    "            shards = []\n",
    "            for shard_idx in range(total_shards):\n",
    "                class_idx = shard_idx % num_classes  \n",
    "                start_idx = (shard_idx // num_classes) * shard_size\n",
    "                end_idx = start_idx + shard_size\n",
    "                shards.append(sorted_indices[class_idx][start_idx:end_idx])\n",
    "\n",
    "            selected_classes = random.sample(range(num_classes), 5)  \n",
    "            selected_shards = [shard for class_idx in selected_classes for shard in shards[class_idx::num_classes]]\n",
    "\n",
    "            for rank, shard in enumerate(selected_shards):\n",
    "                selected_clients = shard\n",
    "                labels1 = [self.train_dataset.targets[idx].item() for idx in selected_clients]\n",
    "\n",
    "            subsets = [torch.utils.data.Subset(self.train_dataset, shard) for shard in selected_shards]  \n",
    "            train_loader = DataLoader(torch.utils.data.ConcatDataset(subsets), batch_size=self.batch_size, shuffle=True)\n",
    "            test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "        elif self.type == \"IID\":\n",
    "            num_classes = 10\n",
    "            shards_per_class = 5\n",
    "            total_shards = num_classes * shards_per_class\n",
    "            shard_size = len(self.indices[0]) // shards_per_class  \n",
    "\n",
    "            sorted_indices = [sorted(self.indices[i]) for i in range(num_classes)]  \n",
    "\n",
    "            shards = []\n",
    "            for shard_idx in range(total_shards):\n",
    "                class_idx = shard_idx % num_classes  # Shard index determines label\n",
    "                start_idx = (shard_idx // num_classes) * shard_size\n",
    "                end_idx = start_idx + shard_size\n",
    "                shards.append(sorted_indices[class_idx][start_idx:end_idx])\n",
    "\n",
    "            for rank, shard in enumerate(shards):\n",
    "                selected_clients = shard\n",
    "                labels1 = [self.train_dataset.targets[idx].item() for idx in selected_clients]\n",
    "\n",
    "            subsets = [torch.utils.data.Subset(self.train_dataset, shard) for shard in shards]  \n",
    "            train_loader = DataLoader(torch.utils.data.ConcatDataset(subsets), batch_size=self.batch_size, shuffle=True)\n",
    "            test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        return train_loader, test_loader\n",
    "print()  \n",
    "loader1 = loader()\n",
    "loader1.get_loader([1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Edge_Server.py\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class ES():\n",
    "    def __init__(self, size, data_loader, device):\n",
    "        self.size = size\n",
    "        self.test_loader = data_loader[1]\n",
    "        self.accuracy = []\n",
    "        self.clients = [None]*size\n",
    "        self.count = 0\n",
    "        self.model =  CNN().to(device)\n",
    "        self.device = device\n",
    "\n",
    "        \n",
    "    def average_weights(self,clients):\n",
    "        for info in clients[1:]:\n",
    "            for key in info:\n",
    "                clients[0][key]=info[key] + clients[0][key]\n",
    "        for key in clients[0]:\n",
    "            clients[0][key]=clients[0][key]/self.size  \n",
    "        weights=clients[0]\n",
    "        return weights\n",
    "\n",
    "    def aggregate(self):\n",
    "        weights_info = self.clients\n",
    "        weights = self.__average_weights(weights_info)\n",
    "        self.model.load_state_dict(weights)\n",
    "        self.CS.ESs[self.CS.count%self.CS.size]=weights\n",
    "        self.CS.count+=1\n",
    "        #test_accuracy = self.__test()\n",
    "        #self.accuracy.append(test_accuracy)\n",
    "        #print('\\n[Global Model]  Test Accuracy: {:.2f}%\\n'.format(test_accuracy * 100.))\n",
    "    \n",
    "    def global_weight(self):\n",
    "        weights = self.model.state_dict()\n",
    "        return weights\n",
    "\n",
    "    def test(self):\n",
    "        test_correct = 0\n",
    "        self.model.eval()\n",
    "        device = self.device\n",
    "        with torch.no_grad():\n",
    "            for data, target in self.test_loader:\n",
    "                data, target = Variable(data).to(device), Variable(target).to(device)\n",
    "                output = self.model(data)\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                test_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        return test_correct / len(self.test_loader.dataset)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hier_Client.py\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "class Client(object):\n",
    "    def __init__(self, rank, data_loader, local_epoch, ES):\n",
    "        seed = 19201077 + 19950920 + rank\n",
    "        torch.manual_seed(seed)\n",
    "        self.accuracy = []\n",
    "        self.rank = rank\n",
    "        self.local_epoch = local_epoch\n",
    "        self.ES=ES\n",
    "        self.test_loader = data_loader[1]\n",
    "        self.train_loader = iter(data_loader[0])\n",
    "\n",
    "    def load_global_model(self):\n",
    "        model = CNN().to(ES.device)\n",
    "        model.load_state_dict(self.ES.model.state_dict()) \n",
    "        return model\n",
    "    \n",
    "    ## Option\n",
    "    # def __test(self, model):\n",
    "    #     test_correct = 0\n",
    "    #     model.eval()\n",
    "    #     device = self.device\n",
    "    #     with torch.no_grad():\n",
    "    #         for data, target in self.test_loader:\n",
    "    #             data, target = Variable(data).to(device),Variable(target).to(device)\n",
    "    #             output = model(data)\n",
    "    #             pred = output.argmax(dim=1, keepdim=True)\n",
    "    #             test_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    #     return test_correct / len(self.test_loader.dataset)\n",
    "\n",
    "    def __train(self, model):\n",
    "        device = self.device\n",
    "        model.train()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                        lr_lambda=lambda epoch: 0.95 ** epoch,\n",
    "                                        last_epoch=-1,\n",
    "                                        verbose=False) \n",
    "        chk=0\n",
    "        for data, target in self.train_loader:\n",
    "            chk+=1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            self.loss = nn.CrossEntropyLoss()(output, target)\n",
    "            self.loss.backward()\n",
    "            optimizer.step()    \n",
    "            if(chk ==1):\n",
    "                break\n",
    "            scheduler.step()\n",
    "        weights=model.state_dict()\n",
    "        \n",
    "        return weights\n",
    "\n",
    "    def run(self):\n",
    "        model = self.__load_global_model(self)\n",
    "        for _ in range(self.local_epoch):\n",
    "            weights = self.train(model=model)\n",
    "   \n",
    "        self.ES.clients[self.ES.count%self.ES.size]=weights\n",
    "        self.ES.count+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def fed_AVG(n_client, n_ES, ES_epoch, epoch, batch_size, device = \"Cuda:0\" ,type = \"IID\"):\n",
    "    print('Initialize Dataset...')\n",
    "    data_loader = loader('mnist', batch_size=batch_size, type = type)    \n",
    "    print('Initialize Edge Servers and Clients...')\n",
    "    ESs = []\n",
    "    clients = [[ None for i in range(n_client)] for j in range(n_ES) ]\n",
    "    for i in range(n_ES):\n",
    "        ESs.append(ES(size = n_client, data_loader = data_loader.get_loader([]), device = device))\n",
    "        for j in range(n_client):\n",
    "            clients[i][j]=Client(rank=j, data_loader=data_loader.get_loader(\n",
    "            random.sample(range(0, 10), 4)\n",
    "            ),local_epoch = epoch,\n",
    "            ES = ESs[i])\n",
    " \n",
    "    # federated learning\n",
    "\n",
    "    for ESe in range(ES_epoch):\n",
    "        print('\\n================== Edge Server Epoch {:>3} =================='.format(ESe + 1))\n",
    "        for ESn in range(n_ES):\n",
    "            print(\"================= Edge Server :\",ESn,\"process =================\")\n",
    "            for c in clients[ESn]:\n",
    "                c.run()\n",
    "                \n",
    "            ESs[ESn].aggregate()\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
