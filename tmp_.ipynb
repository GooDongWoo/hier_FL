{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.config import list_physical_devices\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"Num GPUs Available: \", len(list_physical_devices('GPU')))\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random as rd\n",
    "print(f\"@@@@@@@@ Start time: {datetime.now()}@@@@@@@@@@@\")\n",
    "K_client_num=100\n",
    "S_round=30  #총 라운드 수\n",
    "IS_DATA_CSV_EXIST=1\n",
    "\n",
    "#데이터(MNIST) 불러오고 전처리\n",
    "#데이터 가져오고 합쳐서 70,000개로 합치고 각각 리스트로 나누고 6000개씩 뽑기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "all_x=np.concatenate([x_train, x_test], axis=0)\n",
    "all_y=np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "idx = np.argsort(all_y) #idx는 y_list에서 0부터 9까지의 라벨링된 데이터의 순서대로 정렬한 것의 np.idx_list\n",
    "x_all_sorted = all_x[idx]\n",
    "y_all_sorted = all_y[idx]\n",
    "\n",
    "sorted_x_train=[]#sorted_x_train은 0~9idx에 각각 라벨링이 그것인 x_np 데이터가 들어있음.\n",
    "for i in range(10):\n",
    "    sorted_x_train.append(x_all_sorted[y_all_sorted == i])\n",
    "\n",
    "'''# 각 라벨별로 몇개 있는지\n",
    "print(\"0 :\",len(sorted_x_train[0]),\"1 :\",len(sorted_x_train[1]),\"2 :\",len(sorted_x_train[2]),\"3 :\",len(sorted_x_train[3]),\"4 :\",len(sorted_x_train[4]),\"5 :\",len(sorted_x_train[5]),\n",
    "\"6 :\",len(sorted_x_train[6]),\"7 :\",len(sorted_x_train[7]),\"8 :\",len(sorted_x_train[8]),\"9 :\",len(sorted_x_train[9]))\n",
    "# 라벨0의 shape\n",
    "print(x_all_sorted[y_all_sorted == 0].shape)\n",
    "'''\n",
    "\n",
    "#이제 전부 앞에 6천개만 뽑아서 x_train로 1줄로 세워버리고 전부 나머지는 테스트에다가 박아버리기.\n",
    "x_train=sorted_x_train[0][:6000]\n",
    "x_test=sorted_x_train[0][6000:]\n",
    "for i in range(1,10):\n",
    "    x_train=np.concatenate([x_train,sorted_x_train[i][0:6000]], axis=0)\n",
    "    x_test=np.concatenate([x_test,sorted_x_train[i][6000:]], axis=0)\n",
    "    \n",
    "tmp_int=0\n",
    "y_train=np.zeros(60000,)\n",
    "for i in range(10):\n",
    "    y_train[tmp_int:tmp_int+6000]=i\n",
    "    tmp_int+=6000\n",
    "    \n",
    "tmp_int=0\n",
    "y_test=np.zeros(10000,)\n",
    "for i in range(10):\n",
    "    y_test[tmp_int:tmp_int+(len(sorted_x_train[i])-6000)]=i\n",
    "    tmp_int+=(len(sorted_x_train[i])-6000)\n",
    "\n",
    "# 차원 변환 후, 테스트셋과 학습셋으로 나눔\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 할당하는 과정\n",
    "if not(IS_DATA_CSV_EXIST):\n",
    "    x_per_label=20\n",
    "    data_label_list=[0,1,2,3,4,5,6,7,8,9]\n",
    "    list_added_label_num=[0]*10\n",
    "    last_label_left=-1# 마지막으로 1개밖에 안남은 라벨 숫자\n",
    "    \n",
    "    list_combinational=[]\n",
    "    list_added_label_idx=[]\n",
    "    for iter in range(x_per_label*5):\n",
    "        while 1:\n",
    "            if len(data_label_list)<=1:\n",
    "                last_label_left=list_added_label_num[data_label_list[0]]\n",
    "                what_last_num=data_label_list[0]\n",
    "                break\n",
    "\n",
    "            temp_pick2=rd.sample(data_label_list,2)\n",
    "\n",
    "            if (list_added_label_num[temp_pick2[0]]==x_per_label) or (list_added_label_num[temp_pick2[1]]==x_per_label):\n",
    "                if (list_added_label_num[temp_pick2[0]]==x_per_label):\n",
    "                    data_label_list.remove(temp_pick2[0])\n",
    "                if (list_added_label_num[temp_pick2[1]]==x_per_label):\n",
    "                    data_label_list.remove(temp_pick2[1])\n",
    "                continue\n",
    "            \n",
    "            list_added_label_idx.append([temp_pick2[0]*6000+list_added_label_num[temp_pick2[0]]*300,temp_pick2[1]*6000+list_added_label_num[temp_pick2[1]]*300])\n",
    "            list_combinational.append(temp_pick2)\n",
    "\n",
    "            list_added_label_num[temp_pick2[0]]+=1\n",
    "            list_added_label_num[temp_pick2[1]]+=1\n",
    "            break \n",
    "\n",
    "    #일단 나머지를 라벨 한개짜리로 채우는거\n",
    "    last_label_left_set=-1\n",
    "    if (last_label_left!=-1):\n",
    "        last_label_left_set=(20-last_label_left)/2\n",
    "        for i______ in range(int(last_label_left_set)):\n",
    "            list_added_label_idx.append([what_last_num*6000+list_added_label_num[what_last_num]*300,what_last_num*6000+(list_added_label_num[what_last_num]+1)*300])\n",
    "            list_added_label_num[what_last_num]+=2\n",
    "\n",
    "    print(list_combinational)\n",
    "    print(list_added_label_num)\n",
    "    print(list_added_label_idx)\n",
    "    print(last_label_left_set)\n",
    "else:\n",
    "    list_added_label_idx=[[48000, 36000], [6000, 30000], [0, 48300], [300, 30300], [24000, 30600], [48600, 24300], [12000, 30900], [42000, 12300], [42300, 24600], [31200, 24900], [18000, 54000], [600, 36300], [900, 18300], [25200, 1200], [54300, 1500], [18600, 48900], [6300, 25500], [12600, 31500], [1800, 25800], [31800, 2100], [54600, 12900], [49200, 18900], [32100, 19200], [36600, 6600], [19500, 42600], [54900, 6900], [19800, 2400], [49500, 32400], [42900, 7200], [43200, 13200], [32700, 13500], [33000, 2700], [7500, 55200], [49800, 13800], [7800, 36900], [8100, 33300], [50100, 55500], [8400, 20100], [55800, 50400], [20400, 26100], [8700, 43500], [43800, 56100], [50700, 56400], [56700, 3000], [37200, 20700], [3300, 37500], [3600, 14100], [57000, 14400], [26400, 14700], [21000, 9000], [44100, 15000], [21300, 57300], [3900, 51000], [9300, 51300], [21600, 4200], [21900, 26700], [9600, 51600], [15300, 51900], [9900, 4500], [52200, 33600], [52500, 57600], [10200, 4800], [5100, 33900], [27000, 52800], [57900, 5400], [44400, 15600], [53100, 58200], [22200, 15900], [22500, 44700], [5700, 22800], [58500, 37800], [45000, 27300], [34200, 10500], [53400, 23100], [10800, 45300], [45600, 58800], [27600, 11100], [23400, 27900], [28200, 59100], [38100, 28500], [45900, 38400], [46200, 16200], [11400, 53700], [28800, 11700], [59400, 23700], [46500, 38700], [59700, 46800], [47100, 39000], [47400, 39300], [39600, 47700], [16500, 34500], [29100, 39900], [16800, 34800], [35100, 17100], [17400, 40200], [29400, 40500], [29700, 40800], [17700, 35400], [35700, 41100], [41400, 41700]]\n",
    "    print(f\"#########@@@@@@@@@@@@ DATA EXIST@@@@@@@@@@@@@@#########\")\n",
    "    print(list_added_label_idx)    \n",
    "\n",
    "print(f\"@@@@@@@@ first initialized time: {datetime.now()}@@@@@@@@@@@\")\n",
    "before_time=datetime.now()\n",
    "before_time_round=datetime.now()\n",
    "after_time=datetime.now()\n",
    "\n",
    "#env loop start>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "env_num=0\n",
    "env_start_num=8\n",
    "env_setting=[[10,1],[10,5],[10,20],[50,1],[50,5],[50,20],[600,1],[600,5],[600,20]]\n",
    "for env_num in range(env_start_num,9):\n",
    "    B_batch=env_setting[env_num][0] # 배치 사이즈\n",
    "    E_epoch=env_setting[env_num][1]  # 각 클라이언트마다 몇 에포크 돌릴지\n",
    "    print(f\"###########B_batch={env_setting[env_num][0]}\\n###########E_epoch={env_setting[env_num][1]}\")\n",
    "    ##서버 모델 이니셜라이징\n",
    "    # 모델 구조를 설정\n",
    "    server_model = Sequential()\n",
    "    server_model.add(Conv2D(32, kernel_size=(5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    server_model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "    server_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    #server_model.add(Dropout(0.25))\n",
    "    server_model.add(Flatten())\n",
    "    server_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    #서버 레이어들 정보 요약\n",
    "    #server_model.summary()                                                \n",
    "\n",
    "    # 모델 실행 환경을 설정\n",
    "    server_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # 모델 최적화를 위한 설정 구간\n",
    "    '''serverpath=\"./MNIST_MLP_0.hdf5\"\n",
    "    checkpointer = ModelCheckpoint(filepath=serverpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)'''\n",
    "    # 모델 실행\n",
    "    server_history = server_model.fit(x_train[0:2], y_train[0:2], validation_split=0.25, epochs=1, batch_size=2, verbose=0) #최대한 학습 안할려고 2개만 학습시킴...\n",
    "    #클라이언트 100명 각각 설정하는 것\n",
    "    clients_model=[]\n",
    "    clients_model_w=[]\n",
    "    clients_path=[]\n",
    "    server_w=server_model.get_weights()\n",
    "\n",
    "    history_temp=[[],[]]\n",
    "    clients_history=[]\n",
    "    ##클라이언트i 모델 이니셜라이징\n",
    "    clients_model=Sequential()\n",
    "    clients_model.add(Conv2D(32, kernel_size=(5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    clients_model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "    clients_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    #clients_model.add(Dropout(0.25))\n",
    "    clients_model.add(Flatten())\n",
    "    clients_model.add(Dense(10, activation='softmax'))\n",
    "    #clients_model.summary()                    \n",
    "    \n",
    "    # 모델 실행 환경을 설정\n",
    "    clients_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # 모델 최적화를 위한 설정 구간\n",
    "    '''clients_path.append(\"./MNIST_MLP_\"+str(i)+\".hdf5\")\n",
    "    checkpointer = ModelCheckpoint(filepath=clients_path[i], monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=4)\n",
    "    clients_path'''\n",
    "    \n",
    "    #처음 서버>>>>>>>>>>>>>>>>>클라이언트\n",
    "    for i in range(0,S_round): #i equal -> per round\n",
    "        #각 클라이언트들마다 computations\n",
    "        clients_model_w=[]\n",
    "        for j in range(K_client_num):\n",
    "\n",
    "            clients_model.set_weights(server_w)\n",
    "            #학습\n",
    "            # j equal per clients\n",
    "            clients_history.append(clients_model.fit(np.concatenate([x_train[list_added_label_idx[j][0]:list_added_label_idx[j][0]+300], x_train[list_added_label_idx[j][1]:list_added_label_idx[j][1]+300]], axis=0),np.concatenate([y_train[list_added_label_idx[j][0]:list_added_label_idx[j][0]+300], y_train[list_added_label_idx[j][1]:list_added_label_idx[j][1]+300]], axis=0), validation_split=0.25, epochs=E_epoch, batch_size=B_batch, verbose=0) )\n",
    "            \n",
    "            #clients_model_w에 각 모델들 가중치 저장\n",
    "            clients_model_w.append(clients_model.get_weights())\n",
    "            \n",
    "            if j%10==0:\n",
    "                \n",
    "                print(f\"{j}th client is studying diff time: {datetime.now()-before_time}-------- curr_time :{datetime.now()}\")\n",
    "                before_time=datetime.now()\n",
    "            clear_session()\n",
    "        \n",
    "        #각 클라이언트들의 w를 산술평균해서 서버에다가 주는 과정 1round마다 서버<<<<<<<<<<<<<<<<<<클라이언트\n",
    "        array_temp = []\n",
    "        for j in range(len(clients_model_w[0])):\n",
    "            array_temp.append(clients_model_w[0][j]/K_client_num)\n",
    "            for k in range(1,K_client_num):\n",
    "                array_temp[j]+=(clients_model_w[k][j])/K_client_num\n",
    "                \n",
    "        server_w=(array_temp)\n",
    "        server_model.set_weights(server_w)\n",
    "\n",
    "        #서버의 1round마다의 데이터들의 히스토리를 모으는 과정\n",
    "        history_temp[1].append(server_model.evaluate(x_test, y_test)[1])\n",
    "        print(\"@@@@@\"+str(i+1)+\"th Round Test Accuracy: %.4f\" % (history_temp[-1][-1]))\n",
    "        history_temp[0].append(server_model.evaluate(x_train, y_train)[1])\n",
    "        print(\"@@@@@\"+str(i+1)+\"th Round Train Accuracy: %.4f\" % (history_temp[0][-1]))\n",
    "        print(f\"@@@@@@@@{i}th round diff time: {datetime.now()-before_time_round}@@@@@@@@@@@@@@@@@\")\n",
    "        before_time_round=datetime.now()\n",
    "        \n",
    "    # 테스트 정확도\n",
    "    print(\"\\n Test Accuracy: %.4f\" % (server_model.evaluate(x_test, y_test)[1]))\n",
    "    # 검증셋과 학습셋의 오차를 저장\n",
    "    y_vloss = history_temp[1] #server_history.history['val_loss']\n",
    "    y_loss = history_temp[0]  #server_history.history['loss']\n",
    "\n",
    "    # 그래프로 표현\n",
    "    #plt.yticks(np.arange(0.9,1,0.01))\n",
    "    x_len = np.arange(len(y_loss))\n",
    "    plt.plot(x_len, y_vloss, marker='.', label=f'B={env_setting[env_num][0]} E={env_setting[env_num][1]} \\nTestset_accuracy')\n",
    "    current_values = plt.gca().get_yticks()\n",
    "    plt.gca().set_yticklabels(['{:.4f}'.format(x) for x in current_values])\n",
    "    #plt.plot(x_len, y_loss, marker='.', c=\"blue\", label='Trainset_accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.xlabel('Round')\n",
    "    plt.ylabel('Accuracy')\n",
    "    for x,y in zip(x_len,y_vloss):\n",
    "        if(x%2==0):\n",
    "            label = \"{:.4f}\".format(y)\n",
    "            plt.annotate(label, \n",
    "                        (x,y), # x and y is the points location where we have to label\n",
    "                        textcoords=\"offset points\",\n",
    "                        xytext=(0,10+11*(x%4)), # this for the distance between the points\n",
    "                        ha='center',\n",
    "                        arrowprops=dict(arrowstyle=\"->\", color='green'))\n",
    "    plt.savefig(f'Round={S_round} B={env_setting[env_num][0]} E={env_setting[env_num][1]}Testset_accuracy.png')\n",
    "    clear_session()\n",
    "    plt.show()\n",
    "    np.savetxt(f'Round={S_round} B={env_setting[env_num][0]} E={env_setting[env_num][1]}Testset_accuracy.csv', np.array(y_vloss), delimiter=\",\", fmt=\"%.5f\")\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
